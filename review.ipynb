{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cloud Characteristics</h3>\n",
    "<li>On-demand self-service</li>\n",
    "<li>Networked access</li>\n",
    "<li>Resource pooling</li>\n",
    "<li>Rapid elasticity</li>\n",
    "<li>Measured service</li>\n",
    "<h3>Challenge of earlier DS(networked) implementations</h3>\n",
    "<li>Complexity of implementations</li>\n",
    "<li>Vendor specific solutions</li>\n",
    "<li>Scale of the problem area</li>\n",
    "<h3></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture2: Domain Drivers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>SWARM</h3>\n",
    "<li>Anonymous</li>\n",
    "<li>No leader</li>\n",
    "<li>Team size flexibility</li>\n",
    "<li>Public vs Private</li>\n",
    "<li>Arbitary contributions</li>\n",
    "<li>Social interactions</li>\n",
    "<li>ad hoc use encouraged</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture3: Overview of Distributed and Parallel Computing Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Computing Scaling</h3>\n",
    "<li>Vertical Computing Scaling</li>\n",
    "<ul>\n",
    "    <li>Have faster processors</li>\n",
    "    <li>Limits of fundamental physics/matter (nanoCMOS)</li>\n",
    "</ul>\n",
    "<li>Horizontal Computing Scaling</li>\n",
    "<ul>\n",
    "    <li>Have more processors</li>\n",
    "    <li>Limits of fundamental physics/matter (nanoCMOS)</li>\n",
    "</ul>\n",
    "<h3>Amdahl's Law</h3>\n",
    "$S=\\frac{1}{\\alpha +(1-\\alpha)/N}$<br>\n",
    "<b>It also assumes a fixed problem size– sometimes can’t predict length of time required for jobs,</b><br>\n",
    "<h3>Gustafson-Barsis's Law</h3>\n",
    "$S(N)=\\alpha +N(1-\\alpha)=N-\\alpha(N-1)$<br>\n",
    "<h3>Flynn's Taxonomy</h3>\n",
    "<li>Single Instruction, Single Data stream (SISD)</li>\n",
    "<ul><li>Basic idea of von Neumann computer</li></ul>\n",
    "<li>Single Instruction, Multiple Data streams (SIMD)</li>\n",
    "<ul><li>Parallel computing architecture where many functional units (PU/CPU) perform different operations on the same data</li>\n",
    "<li>Examples include fault tolerant computer architectures, e.g. running multiple error checking processes on same data stream</li></ul>\n",
    "<li>Multiple Instruction, Single Data stream (MISD)</li>\n",
    "<ul><li>focus is on data level parallelism, i.e. many parallel computations, but only a single process (instruction) at a given moment</li>\n",
    "<li>Many modern computers use SIMD instructions, e.g. to improve performance of multimedia use such as for image processing</li></ul>\n",
    "<li>Multiple Instruction, Multiple Data streams (MIMD)</li>\n",
    "<ul><li>machines can be shared memory or distributed memory categories<br>\n",
    "• depends on how MIMD processors access memory\n",
    "<li>Most systems these days operate on MIMD<br>\n",
    "• HPC …</ul></li>\n",
    "<h3>Implicit Parallelism</h3>\n",
    "Supported by parallel languages and parallelizing compilers that take care of identifying parallelism, the scheduling of calculations and the placement of data<br>\n",
    "• Pretty hard to do (more later!)\n",
    "<h3>Explicit Parallelism</h3>\n",
    "<li>In this approach, the programmer is responsible for most of\n",
    "the parallelization effort such as task decomposition,\n",
    "mapping tasks to processors, inter-process communications</li>\n",
    "<li>This approach assumes user is the best judge of how\n",
    "parallelism can be exploited for a particular application (Typically non-trivial to achieve!)</li>\n",
    "<h3>Hardware Parallelisation</h3>\n",
    "<li>Basic CPU</li>\n",
    "<li>Hardware Threading CPU</li>\n",
    "Usually shares arithmetic units. Heavy use of one type of computation can tie up all the available units of the CPU preventing other threads from using them.\n",
    "<li>Multi-Core</li>\n",
    "issue of cache read/write performance and cache coherence.\n",
    "<li>Symmetric Multiprocessing(SMP)</li>\n",
    "Two (or more) identical processors connected to a single, shared main memory, with full access to all I/O devices, controlled by a single OS instance that treats all processors equally.\n",
    "<li>Non-Uniform Memory Access(NUMA)</li>\n",
    "provides speed-up by allowing a processor to access its own local memory faster than non-local memory(!!! decrease inter-processor communication)\n",
    "<h3>Operating System Parallelism Approaches</h3>\n",
    "Most modern multi-core operating systems support different forms of parallelisation<br>\n",
    "<li>Compute parallelism</li>\n",
    "<ul>\n",
    "    <li>Processes</li>\n",
    "    <li>Threads</li>\n",
    "</ul>\n",
    "<li>Data parallelsim</li>\n",
    "<b>Data parallelism is parallelization across multiple processors in parallel computing environments. It focuses on distributing the data across different nodes, which operate on the data in parallel.</b>\n",
    "<ul>\n",
    "    <li>Caching(cache coherency)</li>\n",
    "    <li>OS implies on \"a\" computer</li>\n",
    "</ul>\n",
    "<h3>Software Parallelism Approach</h3>\n",
    "<b>Key issues that need to be tackled:</b><br>\n",
    "<li>Deadlock</li>\n",
    "<li>Livelock</li>\n",
    "<h3>Message Passing Interface</h3>\n",
    "<li>Key MPI functions</li>\n",
    "<ul>\n",
    "<li>MPI_Init :initiate MPI computation</li>\n",
    "<li>MPI_Finalize :terminate computation</li>\n",
    "<li>MPI_COMM_SIZE :determine number of processors</li>\n",
    "<li>MPI_COMM_RANK :determine my process identifier</li>\n",
    "<li>MPI_SEND :send a message</li>\n",
    "<li>MPI_RECV :receive a message</li>\n",
    "<li>MPI_BCAST:distributes data from one process (the root) to all others in a communicator.</li>\n",
    "<li>MPI_REDUCE:combines data from all processes in communicator and returns it to one process.</li>\n",
    "</ul>\n",
    "<h3>Data Parallelism Approaches</h3>\n",
    "<li>Distributed data</li>\n",
    "CAP Throrem - Consistency, Availability , Partition tolerance<br>\n",
    "ACID(Atomic, Consistency, Isolation, Durability) -> <b>BASE(Basically Available系统能够基本运行、一直提供服务, Soft-state系统不要求一直保持强一致状态, Eventual consistency系统需要在某一时刻后达到一致性要求)</b><br>\n",
    "<li>Distributed File System</li>\n",
    "- e.g. Hadoop\n",
    "<h3>Erroneous Assumptions of Distributed System</h3>\n",
    "<ol>\n",
    "<li>The network is reliable</li>按顺序无损到达\n",
    "<li>Latency is zero</li>\n",
    "<li>Bandwidth is infinite</li>\n",
    "<li>The network is secure</li>\n",
    "<li>Topology doesn't change</li>\n",
    "<li>There is one administrator</li>\n",
    "Firewall changes, server reconfigurations, services, access control\n",
    "<li>Transport cost is zero</li>\n",
    "<li>The network is homogeneous</li>\n",
    "<li>Time is ubiquitous</li>\n",
    "TIme is same across all computers in network\n",
    "</ol>\n",
    "<h3>Design stages of parallel programs</h3>\n",
    "<li>Partitioning</li>\n",
    "<li>Communication</li>\n",
    "<li>Agglomeration</li>集聚\n",
    "<li>Mapping / Scheduling</li>\n",
    "<h3>Partition model</h3>\n",
    "<li>Master Worker/Slave Model</li>master分发任务，归并任务\n",
    "<li>Single-Program Multiple-Data</li>每个进程以一样的代码处理不一样的数据\n",
    "<li>Data pipelining</li>Suitable for applications involving multiple stages of execution, that typically operate on large number of data sets.\n",
    "<li>Divide and Conquer</li>Master-worker/taskfarming is like divide and conquer with master doing both split and join operation\n",
    "<li>Speculative Parallelism</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture4: MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>MPI Functions</h3>\n",
    "<ul>\n",
    "<li>MPI_Init :initiate MPI computation</li>\n",
    "<li>MPI_Finalize :terminate computation</li>\n",
    "<li>MPI_COMM_SIZE :determine number of processors</li>\n",
    "<li>MPI_COMM_RANK :determine my process identifier</li>\n",
    "<li>MPI_SEND :send a message</li>\n",
    "<li>MPI_RECV :receive a message</li>\n",
    "<li>MPI_BCAST:distributes data from one process (the root) to all others in a communicator.</li>\n",
    "<li>MPI_REDUCE:combines data from all processes in communicator and returns it to one process.</li>\n",
    "</ul>\n",
    "<h3>Supercomputers</h3>\n",
    "it means any single computer system(itself a contested term) that has exceptional processing power for its time.\n",
    "<h3>High Performance Computing</h3>\n",
    "High-performance computing (HPC) is any computer system whose architecture allows for above average performance. A system that is one of the most powerful in\n",
    "the world, but is poorly designed, could be a \"supercomputer\".\n",
    "<h3>Clustered computing</h3>\n",
    "Clustered computing is when two or more computers serve a single resource. This improves performance and provides redundancy; typically a collection of smaller computers strapped together with a high-speed <b>local network</b>.\n",
    "<h3>Parallel computing</h3>\n",
    "Parallel computing refers to the submission of jobs or processes over multiple processors and by splitting up the data or tasks between them.\n",
    "<h3>Research computing</h3>\n",
    "Research computing is the software applications used by a research community to aid research.\n",
    "<h3>Limitations of Parallel Compution</h3>\n",
    "<li>Synchronisation and atomic operations causes loss of performance, communication latency.</li>\n",
    "<h3>Slurms</h3>\n",
    "Simple Linux Utility for Resource Management,now simply called Slurm Workload Manager, also uses batch script where are very similar in intent and style to PBS scripts.<br>\n",
    "Submitting and running jobs is a relatively straight-forward process consisting of:<br>\n",
    "1) Setup and launch<br>\n",
    "2) Job Control, Monitor results<br>\n",
    "3) Retrieve results and analyse<br>\n",
    "<b>Script Example</b><br>\n",
    "#SBATCH --time=01:00:00<br>\n",
    "#SBATCH --nodes=1<br>\n",
    "#SBATCH --ntasks-per-node=1<br>\n",
    "<h3>multithreading</h3>\n",
    "a master thread forks a number of sub-threads and divides tasks between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture5: NeCTAR/Unimelb Cloud & Scripting & Git"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cloud Computing</h3>\n",
    "Cloud computing is a model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction.<br>\n",
    "<b>Essential Characteristics</b><br>\n",
    "<li>On-demand self-service</li>\n",
    "<li>Broad network access</li>\n",
    "<li>Resource pooling</li>\n",
    "<li>Rapid elasticity</li>\n",
    "<li>Measured service</li>\n",
    "<h3>Cloud Deployment Models</h3>\n",
    "<li><b>Public Cloud</b></li>\n",
    "<ul>\n",
    "<li><b>Pros</b><br>\n",
    "Utility computing<br>\n",
    "Can focus on core business<br>\n",
    "Cost-effective<br>\n",
    "“Right-sizing”<br>\n",
    "Democratisation of computing<br></li>\n",
    "<li><b>Cons</b><br>\n",
    "Security<br>\n",
    "Loss of control<br>\n",
    "Possible lock-in<br>\n",
    "Dependency of Cloud provider continued existence</li>\n",
    "</ul>\n",
    "<li><b>Private Cloud</b></li>\n",
    "<ul>\n",
    "<li><b>Pros</b><br>\n",
    "Control<br>\n",
    "Consolidation of resources<br>\n",
    "Easier to secure<br>\n",
    "More trust<br></li>\n",
    "<li><b>Cons</b><br>\n",
    "Relevance to core business?<br>\n",
    "-----e.g. Netflix moved to Amazon<br>\n",
    "Staff/management overheads<br>\n",
    "Hardware obsolescence<br>\n",
    "Over/under utilisation challenges<br></li>\n",
    "</ul>\n",
    "<li><b>Hybrid Cloud</b></li>\n",
    "<ul>\n",
    "    <li><b>Examples</b></li>\n",
    "    Eucalyptus, VMWare vCloud Hybrid Service\n",
    "<li><b>Pros</b><br>\n",
    "Cloud-bursting<br>\n",
    "Use private cloud, but burst into public cloud when needed\n",
    "</li>\n",
    "<li><b>Cons</b><br>\n",
    "How do you move data/resources when needed?<br>\n",
    "How to decide (in real time?) what data can go to public cloud?<br>\n",
    "Is the public cloud compliant with PCI-DSS (Payment Card Industry – Data Security Standard)?<br></li>\n",
    "</ul>\n",
    "<h3>Delivery Models</h3>\n",
    "<li>SaaS</li>Gmail\n",
    "<li>Paas</li>MIcrosoft Azure/Amazon Elastic Aneka\n",
    "<li>Iaas</li>Nectar\n",
    "<h3>NeCTAR</h3>\n",
    "National eResearch Collaboration Tools and Resources\n",
    "(based on OpenStack)\n",
    "<h3>Automation</h3>\n",
    "Automation is the mechanism used to make servers reach a desirable state, previously defined by provisioning scripts using tool-specific languages and features. \n",
    "<li>Provides a record of what you did</li>\n",
    "<li>Codifies knowledge about the system</li>\n",
    "<li>Makes process repeatable</li>\n",
    "<li>Makes it programmable – “Infrastructure as Code”</li>\n",
    "<h3>Scripting Tools</h3>\n",
    "<li>Cloud-focused</li>\n",
    "Used to interact with Cloud services.\n",
    "<li>Shell script</li>\n",
    "Bash/Perl\n",
    "<li><b>Configuration management (CM) tools</b></li>\n",
    "Configuration management refers to the process of systematically handling changes to a system in a way that it maintains integrity over time.\n",
    "<h3>Ansible</h3>\n",
    "An automation tool for configuring and managing computers\n",
    "<h3>Ansible:Features</h3>\n",
    "<li>Easy to learn</li>\n",
    "<li>Minimal requirements</li>\n",
    "<li>Idempotent</li>\n",
    "<li>Extensible</li>\n",
    "<h3>GIT</h3>\n",
    "<li>merge</li>\n",
    "目标分支改变加到自己分支中\n",
    "<li>rebase</li>\n",
    "回退到共同分支，执行目标分支changes（打到目标分支状态），执行自己分支changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
